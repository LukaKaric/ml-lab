{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy2niZFOnEpMSZ0jOXADWK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukaKaric/ml-lab/blob/main/untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq-ZKwaJHBBO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"diabetes_risk_prediction_dataset.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "U12jP5BrHLm-",
        "outputId": "e44fb54c-c071-4879-a55f-0fc785c58109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'diabetes_risk_prediction_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d2b74283d9d4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"diabetes_risk_prediction_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes_risk_prediction_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.replace('Female',0)\n",
        "data=data.replace('Male',1)\n",
        "data=data.replace('No',0)\n",
        "data=data.replace('Yes',1)\n",
        "data=data.replace('Negative',0)\n",
        "data=data.replace('Positive',1)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "Bxl5lCoDHNlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cor = data.corr()\n",
        "sns.heatmap(cor, annot=True)"
      ],
      "metadata": {
        "id": "HBjZ6FGBHN1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cor = data.corr()['class']\n",
        "display(cor)"
      ],
      "metadata": {
        "id": "uT1A7MlvHP3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=[\"Age\", \"muscle stiffness\", \"Genital thrush\", \"Itching\", \"Obesity\"], inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kwPv4RGlHP5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y=data[\"class\"]\n",
        "X = data.drop(columns=[\"class\"])\n",
        "X.head()\n",
        "print(X.shape)\n",
        "y = pd.get_dummies(y, drop_first=True)\n",
        "y.head()\n",
        "X.columns = X.columns.astype(str)\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape)"
      ],
      "metadata": {
        "id": "27zkjnjaHP7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "rmse=metrics.mean_squared_error(ytrain,ytest)"
      ],
      "metadata": {
        "id": "NIXlgxF8HUCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "ks = [x for x in range(1, 15, 2)]\n",
        "\n",
        "models = [KNeighborsClassifier(n_neighbors=k) for k in ks]\n",
        "\n",
        "for model in models:\n",
        "  print(f\"k = {model.get_params()['n_neighbors']}\")\n",
        "  model.fit(Xtrain, ytrain)\n",
        "  y_pred = model.predict(Xtest)\n",
        "  print(classification_report(ytest, y_pred))"
      ],
      "metadata": {
        "id": "uOZXjUAdHWiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model=LogisticRegression()\n",
        "model.fit(Xtrain,ytrain)\n",
        "y_pred = model.predict(Xtest)\n",
        "print(classification_report(ytest, y_pred))"
      ],
      "metadata": {
        "id": "fIZyE8aOHX3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from  sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "theta = np.random.randn(2)\n",
        "\n",
        "epohe = 7\n",
        "eta=0.01\n",
        "\n",
        "print(theta)\n",
        "#profesor\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def gradient_descent(model, X, y, epoch, batch_size, losses, lr):\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  for i in range(0, len(X), batch_size):\n",
        "    # uzimamo mini-batch-eve\n",
        "    x_batch = X[i:i+batch_size]\n",
        "    y_batch = y[i:i+batch_size]\n",
        "\n",
        "    # racunamo predikciju i gresku modela\n",
        "    y_pred = model[0] + model[1] * x_batch\n",
        "    mse = mean_squared_error(y_batch, y_pred)\n",
        "\n",
        "    # logujemo gresku modela\n",
        "    losses[len(losses)+1] = mse\n",
        "\n",
        "    # racunamo gradjent na mini-batch-u\n",
        "    grad0 = -2 * np.mean(y_batch - y_pred)\n",
        "    grad1 = -2 * np.mean(x_batch * (y_batch - y_pred))\n",
        "\n",
        "    # primenjujemo gradijent na model\n",
        "    model[0] -= lr * grad0\n",
        "    model[1] -= lr * grad1\n",
        "\n",
        "  # racunamo gresku modela na celom skupu\n",
        "  y_pred = model[0] + model[1] * X\n",
        "  return mean_squared_error(y, y_pred)\n",
        "# Hiperparametri:\n",
        "epohe = 50\n",
        "eta = 0.01\n",
        "#mesanje podataka\n",
        "data = list(zip(X, y))\n",
        "random.shuffle(data)\n",
        "X, y = zip(*data)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#obucavanje gdom\n",
        "losses = {}\n",
        "epoch_losses = {}\n",
        "\n",
        "batch_size = len(X) # zbog ovoga je GD\n",
        "\n",
        "for epoch in range(epohe):\n",
        "  loss = gradient_descent(gd_model, X, y, epoch, batch_size=batch_size, losses=losses, lr=eta)\n",
        "  epoch_losses[epoch] = loss\n",
        "sns.lineplot(x=losses.keys(), y=losses.values())\n",
        "plt.show()\n",
        "sns.lineplot(x=epoch_losses.keys(), y=epoch_losses.values())\n",
        "plt.show()\n",
        "print(f\"Best MSE: {min(epoch_losses.values())}\")\n",
        "#obucavanje sgdom\n",
        "losses = {}\n",
        "epoch_losses = {}\n",
        "\n",
        "batch_size = 1 # zbog ovoga je SGD\n",
        "import random\n",
        "for epoch in range(epohe):\n",
        "  loss = gradient_descent(sgd_model, X, y, epoch, batch_size=batch_size, losses=losses, lr=eta)\n",
        "  epoch_losses[epoch] = loss\n",
        "sns.lineplot(x=losses.keys(), y=losses.values())\n",
        "plt.show()\n",
        "sns.lineplot(x=epoch_losses.keys(), y=epoch_losses.values())\n",
        "plt.show()\n",
        "print(f\"Best MSE: {min(epoch_losses.values())}\")\n",
        "#obucavanje mgdbom\n",
        "losses = {}\n",
        "epoch_losses = {}\n",
        "\n",
        "batch_size = 32 # zbog ovoga je MBGD\n",
        "import random\n",
        "for epoch in range(epohe):\n",
        "  loss = gradient_descent(mbgd_model, X, y, epoch, batch_size=batch_size, losses=losses, lr=eta)\n",
        "  epoch_losses[epoch] = loss\n",
        "\n",
        "sns.lineplot(x=losses.keys(), y=losses.values())\n",
        "plt.show()\n",
        "sns.lineplot(x=epoch_losses.keys(), y=epoch_losses.values())\n",
        "plt.show()\n",
        "print(f\"Best MSE: {min(epoch_losses.values())}\")\n",
        "     #potpuno ispravan postupa treniranja\n",
        "     from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = list(zip(X, y))\n",
        "train, test = train_test_split(data, test_size=0.2, shuffle=True)\n",
        "valid, test = train_test_split(test, test_size=0.5, shuffle=True)\n",
        "\n",
        "print(len(train), len(valid), len(test))\n",
        "print(valid)\n",
        "#prebacivanje skupova u ispravan pormat\n",
        "\n",
        "trainX, trainY = zip(*train)\n",
        "trainX = np.array(list(trainX))\n",
        "trainY = np.array(list(trainY))\n",
        "\n",
        "testX, testY = zip(*test)\n",
        "testX = np.array(list(testX))\n",
        "testY = np.array(list(testY))\n",
        "\n",
        "validX, validY = zip(*valid)\n",
        "validX = np.array(list(validX))\n",
        "validY = np.array(list(validY))\n",
        "\n",
        "print(validX)\n",
        "#efinicija hiperparamatera\n",
        "\n",
        "max_epochs = 10000\n",
        "patience = 5\n",
        "tolerance = 1e-5\n",
        "eta = 0.001\n",
        "batch_size=32\n",
        "#Definicija funkcije za racunanje gubitka modela na datim podacima\n",
        "def model_loss(model, X, y):\n",
        "  y_pred = model[0] + model[1] * X\n",
        "  return mean_squared_error(y, y_pred)\n",
        "  #obucavanje mbgdom\n",
        "  losses = {}\n",
        "train_losses = {}\n",
        "valid_losses = {}\n",
        "best_loss = float(\"inf\")\n",
        "model = np.zeros(2)\n",
        "\n",
        "count = 0\n",
        "for epoch in range(max_epochs):\n",
        "  train_loss = gradient_descent(model, trainX, trainY, epoch, batch_size=batch_size, losses=losses, lr=eta)\n",
        "  train_losses[epoch] = train_loss\n",
        "  valid_loss = model_loss(model, validX, validY)\n",
        "  valid_losses[epoch] = valid_loss\n",
        "\n",
        "  # provera za rano zaustavljanje\n",
        "  if valid_loss >= best_loss - tolerance:\n",
        "    count += 1\n",
        "  else:\n",
        "    count = 0\n",
        "    best_loss = valid_loss\n",
        "\n",
        "  if count == patience:\n",
        "    break\n",
        "\n",
        "sns.lineplot(x=losses.keys(), y=losses.values())\n",
        "plt.show()\n",
        "sns.lineplot(data=train_losses,x=train_losses.keys(), y=train_losses.values(), legend=\"auto\")\n",
        "sns.lineplot(data=valid_losses,x=valid_losses.keys(), y=valid_losses.values(), legend=\"auto\")\n",
        "plt.show()\n",
        "print(f\"Best validation MSE: {min(valid_losses.values())}\")\n",
        "print(f\"Best train MSE: {min(train_losses.values())}\")\n",
        "\n",
        "test_loss = model_loss(model, testX, testY)\n",
        "print(f\"Test MSE: {test_loss}\")"
      ],
      "metadata": {
        "id": "3Nzwv8XyHZiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_slucajeva=80\n",
        "X=6*np.random.rand(n_slucajeva)\n",
        "y=5+8*X+np.random.randn(n_slucajeva)\n",
        "\n",
        "space=np.linspace(0,5,)\n",
        "\n",
        "y_line = theta[0] + (theta[1]*space)\n",
        "sns.scatterplot(x=X,y=y)\n",
        "sns.lineplot(x=space,y=y_line)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "mhOcrx6JHb0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figures=[]\n",
        "losses={}\n",
        "for epoch in range(epohe):\n",
        "  y_pred=theta[0]+theta[1]*X\n",
        "\n",
        "  mse=mean_squared_error(y,y_pred)\n",
        "\n",
        "  grad0=-2*np.mean(y-y_pred)\n",
        "  grad1=-2*np.mean(X*(y-y_pred))\n",
        "\n",
        "  theta[0]-=eta*grad0\n",
        "  theta[1]-=eta*grad1\n",
        "\n",
        "  y_line=theta[0]+theta[1]*space\n",
        "  y_pred=theta[0]+theta[1]*X\n",
        "  mse=mean_squared_error(y,y_pred)\n",
        "  sns.scatterplot(x=X,y=y)\n",
        "  sns.lineplot(x=space,y=y_line)\n",
        "  plt.title(f\"{epoch+1}.epoha\")\n",
        "  plt.suptitle(f\"MSE:{mse}\")\n",
        "  fig,_=plt.subplots()\n",
        "  figures.append(fig)\n",
        "  losses[epoch+1] = mse\n",
        "plt.clf()\n"
      ],
      "metadata": {
        "id": "wWU7cUz5HdYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}